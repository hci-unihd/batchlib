{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GaussianBlur1D(torch.nn.Module):\n",
    "    def __init__(self, sigma, sigma_cutoff=None, cutoff=None, eps=1e-10, normalize_weight=True, normalize_border=True):\n",
    "        if cutoff is None:\n",
    "            if sigma_cutoff is None:\n",
    "                sigma_cutoff = 3  # by default, use 3 sigma\n",
    "            cutoff = int(np.ceil(sigma * sigma_cutoff))\n",
    "        else:\n",
    "            assert sigma_cutoff is None\n",
    "        super(GaussianBlur1D, self).__init__()\n",
    "        dx = torch.arange(-cutoff, cutoff + 1, dtype=torch.float)\n",
    "        w = torch.exp(-(dx ** 2) / (2 * sigma ** 2))\n",
    "        if normalize_weight:\n",
    "            w /= w.sum()\n",
    "        self.register_buffer('w', w)\n",
    "        self.cutoff = cutoff\n",
    "        self.eps = eps\n",
    "        self.normalize_border = normalize_border\n",
    "\n",
    "    @functools.lru_cache(maxsize=8)\n",
    "    def border_normalizer(self, shape):\n",
    "        x = torch.ones(1, 1, *shape[2:]).to(self.w.device)\n",
    "        x = F.conv1d(x, self.w[None, None], padding=self.cutoff)\n",
    "        x /= max(x.max(), self.eps)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.conv1d(x, self.w[None, None], padding=self.cutoff)\n",
    "        if self.normalize_border:\n",
    "            return x / self.border_normalizer(x.shape)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class GaussianBlur2D(GaussianBlur1D):\n",
    "    @functools.lru_cache(maxsize=8)\n",
    "    def border_normalizer(self, shape):\n",
    "        x = torch.ones(shape).to(self.w.device)\n",
    "        x = F.conv2d(x, self.w[None, None, :, None].repeat(shape[1], 1, 1, 1),\n",
    "                     padding=(self.cutoff, 0), groups=shape[1])\n",
    "        x = F.conv2d(x, self.w[None, None, None, :].repeat(shape[1], 1, 1, 1),\n",
    "                     padding=(0, self.cutoff), groups=shape[1])\n",
    "        x /= max(x.max(), self.eps)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_channels = x.shape[1]\n",
    "        self.w = self.w.to(x.device)\n",
    "        x = F.conv2d(x, self.w[None, None, :, None].repeat(n_channels, 1, 1, 1),\n",
    "                     padding=(self.cutoff, 0), groups=n_channels)\n",
    "        x = F.conv2d(x, self.w[None, None, None, :].repeat(n_channels, 1, 1, 1),\n",
    "                     padding=(0, self.cutoff), groups=n_channels)\n",
    "        if self.normalize_border:\n",
    "            return x / self.border_normalizer(x.shape)\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "        \n",
    "def blur(arr, sigma):\n",
    "    if arr.ndim == 3:\n",
    "        return GaussianBlur2D(sigma)(torch.from_numpy(arr).float()[None]).cpu().numpy()[0]\n",
    "    if arr.ndim == 2:\n",
    "        return GaussianBlur2D(sigma)(torch.from_numpy(arr).float()[None, None]).cpu().numpy()[0, 0]\n",
    "    assert False, f'{arr.shape}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illumination correction from test images (Unused in favour of method below)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tifffile\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "offset = 550\n",
    "channel_mapping=dict(DAPI='DAPI', GFP='WF_GFP', Cy5='WF_Cy5', TRITC='TRITC')\n",
    "\n",
    "correctors_dict = {}\n",
    "for f in glob('/home_sdc/rremme_tmp/Datasets/covid_antibodies/barrel_correction/current/*.tif'):\n",
    "    channel = channel_mapping[re.search('_([A-Za-z0-9]*)\\.tif', f).groups()[0]]\n",
    "    print(channel)\n",
    "    img = tifffile.imread(f).astype(np.float32) - offset\n",
    "    img /= np.mean(img)\n",
    "    img = blur(img, sigma=5)\n",
    "    correctors_dict[channel] = img\n",
    "    \n",
    "for key, img in correctors_dict.items():\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.colorbar()\n",
    "    plt.title(key)\n",
    "    plt.show()\n",
    "    \n",
    "# with h5py.File('barrel_corrector_1024x1024.h5', 'w') as f:\n",
    "#     for key, img in correctors_dict.items():\n",
    "#         f.create_dataset(key, data=img, compression='gzip')\n",
    "#         f[key].attrs['offset'] = offset\n",
    "#     f.create_dataset('DIA', data=np.ones_like(img), compression='gzip')\n",
    "#     f['DIA'].attrs['offset'] = 0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import tifffile\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "offset = 550\n",
    "channel_mapping=dict(DAPI='DAPI', GFP='WF_GFP', Cy5='WF_Cy5', TRITC='TRITC')\n",
    "\n",
    "correctors_dict = {}\n",
    "for f in glob('/home_sdc/rremme_tmp/Datasets/covid_antibodies/unevenIllumination_*.tif'):\n",
    "    channel = channel_mapping[re.search('_([A-Za-z0-9]*)\\.tif', f).groups()[0]]\n",
    "    print(channel)\n",
    "    img = tifffile.imread(f).astype(np.float32) - offset\n",
    "    img /= np.mean(img)\n",
    "    img = blur(img, sigma=5)\n",
    "    correctors_dict[channel] = img\n",
    "    \n",
    "for key, img in correctors_dict.items():\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.colorbar()\n",
    "    plt.title(key)\n",
    "    plt.show()\n",
    "    \n",
    "with h5py.File('barrel_corrector.h5', 'w') as f:\n",
    "    for key, img in correctors_dict.items():\n",
    "        f.create_dataset(key, data=img, compression='gzip')\n",
    "        f[key].attrs['offset'] = offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Barrel Corrector from raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some paths:\n",
    " \n",
    "# FIXME: set this to the location of your batchlib misc folder\n",
    "misc_folder = '../batchlib/misc/' \n",
    "\n",
    "# FIXME: set this to a directory to save the plate-wise averages in\n",
    "average_dir = '../batchlib/misc/plate_stats/'\n",
    "\n",
    "# FIXME: set this to a list of paths to the plates from which you want to compute the barrel corrector\n",
    "plate_dirs = glob('/export/home/rremme/Datasets/antibodies/covid-data-vibor/*')\n",
    "\n",
    "offset = 550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute averages and variances for all plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchlib.preprocessing.preprocess import parse_channel_names\n",
    "from batchlib.outliers.outlier import get_outlier_predicate\n",
    "from batchlib.util.io import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def average_over_tiff_directory(directory, normalize_imgs=False):\n",
    "    \"\"\"computes the average image of all *.tiff files in the specified directory\"\"\"\n",
    "    class Config:\n",
    "        misc_folder = misc_folder\n",
    "        input_folder = directory\n",
    "    outlier_predicate = get_outlier_predicate(Config)\n",
    "    total = 0\n",
    "    n = 0\n",
    "    perc = []\n",
    "    for filename in tqdm(list(filter(lambda f: f.endswith('.tiff') and not outlier_predicate(f) == 1,\n",
    "                                     os.listdir(directory)))):\n",
    "        img = tifffile.imread(os.path.join(directory, filename)).astype(np.int64)\n",
    "        if normalize_imgs:\n",
    "            img -= 550\n",
    "            img = img.astype(np.float32)\n",
    "            img /= np.mean(img, axis=(-1, -2), keepdims=True)\n",
    "        total += img\n",
    "        n += 1\n",
    "    return total / n\n",
    "\n",
    "def average_and_variance_over_tiff_directory(directory):\n",
    "    \"\"\"computes the pixelwise average and variance over all *.tiff files in the specified directory\"\"\"\n",
    "    avg = average_over_tiff_directory(directory)\n",
    "    class Config:\n",
    "        misc_folder = misc_folder\n",
    "        input_folder = directory\n",
    "    outlier_predicate = get_outlier_predicate(Config)\n",
    "    total = 0\n",
    "    n = 0\n",
    "    perc = []\n",
    "    for filename in tqdm(list(filter(lambda f: f.endswith('.tiff') and not outlier_predicate(f) == 1,\n",
    "                                     os.listdir(directory)))):\n",
    "        total += np.abs(avg - tifffile.imread(os.path.join(directory, filename)).astype(np.int64))**2\n",
    "        n += 1\n",
    "    return avg, total / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "import time\n",
    "\n",
    "os.makedirs(average_dir, exist_ok=True)\n",
    "\n",
    "# # this is a hack to get the most recent plates\n",
    "# def get_days_since_change(file):\n",
    "#     return (time.time() - os.path.getmtime(file)) / 60 / 60 / 24 \n",
    "# plate_dirs = list(filter(lambda x: get_days_since_change(x) < 1, plate_dirs))\n",
    "\n",
    "failed = []\n",
    "for plate_dir in tqdm(plate_dirs):\n",
    "    try:\n",
    "        if plate_dir.endswith(('_IgA', '_IgG')):\n",
    "            continue\n",
    "        plate_name = os.path.basename(plate_dir)\n",
    "        print(plate_name)\n",
    "        avg, var = average_and_variance_over_tiff_directory(plate_dir)\n",
    "        channels = parse_channel_names(list(filter(lambda f: f.endswith('.tiff'), os.listdir(plate_dir)))[0])\n",
    "        result_file = os.path.join(average_dir, plate_name + '.h5')\n",
    "\n",
    "        # get outlier info \n",
    "        class Config:\n",
    "            misc_folder = misc_folder\n",
    "            input_folder = plate_dir\n",
    "        outlier_predicate = get_outlier_predicate(Config)\n",
    "        n_total = len(list(os.listdir(plate_dir)))\n",
    "        n_outlier = len(list(filter(lambda f: f.endswith('.tiff') and outlier_predicate(f) == 1, os.listdir(plate_dir))))\n",
    "        n_non_outlier = n_total - n_outlier\n",
    "\n",
    "        with open_file(result_file, 'a') as f:\n",
    "            for i, channel in enumerate(channels):\n",
    "                if 'average' in f.keys():\n",
    "                    del f['average']\n",
    "                write_image(f, f'{channel}/average', avg[i])\n",
    "                write_image(f, f'{channel}/variance', var[i])\n",
    "                f[f'{channel}'].attrs['n_total'] = n_total\n",
    "                f[f'{channel}'].attrs['n_outlier'] = n_outlier\n",
    "                f[f'{channel}'].attrs['n_non_outlier'] = n_non_outlier\n",
    "        print(plate_name)\n",
    "    except Exception:\n",
    "        failed.append(plate_dir)\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "print(f'{len(failed)} plates failed: {failed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group averages by image shape and select the one we want to use: (1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "avgs_by_shape = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "for file in tqdm(glob(os.path.join(average_dir, '*.h5'))):\n",
    "    plate_name = os.path.basename(file)[:-3]\n",
    "    print(plate_name)\n",
    "    with open_file(file, 'r') as f:\n",
    "        for i, channel in enumerate(f.keys()):\n",
    "            img = read_image(f, f'{channel}/average')\n",
    "            extra_info = dict(f[channel].attrs)\n",
    "            extra_info['plate'] = plate_name\n",
    "            avgs_by_shape[img.shape][channel][plate_name] = (img, extra_info)\n",
    "            #f[f'{channel}'].attrs['n_total'] = n_total\n",
    "            #f[f'{channel}'].attrs['n_outlier'] = n_outlier\n",
    "            #f[f'{channel}'].attrs['n_non_outlier'] = n_non_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('available shapes: ', avgs_by_shape.keys())\n",
    "avgs_by_channel = avgs_by_shape[(1024, 1024)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meta_avg_dict = dict()  # maps channels to blurred average of averages\n",
    "for channel, avgs_by_plate in avgs_by_channel.items():\n",
    "    print(channel)\n",
    "    if len(avgs_by_plate) == 0:\n",
    "        print('nope..')\n",
    "        continue\n",
    "    stacked_avgs = np.stack(list(map(lambda x: x[0], avgs_by_plate.values())))\n",
    "    stacked_avgs -= offset\n",
    "    stacked_avgs /= stacked_avgs.mean(axis=(-1, -2), keepdims=True)\n",
    "    weights = np.array(list(map(lambda x: x[1]['n_non_outlier'], avgs_by_plate.values()))).astype(np.float32)\n",
    "    weights /= np.sum(weights)\n",
    "    avg_of_avgs = (stacked_avgs * weights[:, None, None]).sum(0)\n",
    "    meta_avg_dict[channel] = blur(avg_of_avgs, 30) # TODO: sigma=30 is currently hard coded\n",
    "    #image_interact(meta_avg_dict[channel], colorbar=True, vmin=0, title=channel)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# This was just to save some images to look at, if I remember correctly\n",
    "out_dir = 'flatfield_tiffs'\n",
    "for shape_string in ['1024x1024', '930x1024']:\n",
    "    os.makedirs(os.path.join(out_dir, shape_string))\n",
    "    \n",
    "for key, img in meta_avg_dict.items():\n",
    "    tifffile.imsave(os.path.join(out_dir, f'1024x1024/divisor_{key}.tiff'), data=img, shape=img.shape, dtype=np.float32)\n",
    "    img = img.copy()\n",
    "    img = img[1024-930:, :]\n",
    "    tifffile.imsave(os.path.join(out_dir, f'930x1024/divisor_{key}.tiff'), data=img, shape=img.shape, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: adjust the save locations to your liking\n",
    "\n",
    "with h5py.File('barrel_corrector_1024x1024_new-microscope.h5', 'w') as f:\n",
    "    f.attrs['image_shape'] = [1024, 1024] \n",
    "    for key, img in meta_avg_dict.items():\n",
    "        f.create_dataset(key, data=img, compression='gzip')\n",
    "        f[key].attrs['offset'] = offset\n",
    "    \n",
    "with h5py.File('barrel_corrector_930x1024_new-microscope.h5', 'w') as f:\n",
    "    f.attrs['image_shape'] = [930, 1024] \n",
    "    for key, img in meta_avg_dict.items():\n",
    "        f.create_dataset(key, data=img[1024-930:], compression='gzip')\n",
    "        f[key].attrs['offset'] = offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything below is not neccessary and probably does not work anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plate_dir = '/export/home/rremme/Datasets/antibodies/covid-data-vibor/20200417_132123_311/'\n",
    "channels = parse_channel_names(list(filter(lambda f: f.endswith('.tiff'), os.listdir(plate_dir)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('../batchlib/misc/barrel_correctors/barrel_corrector_1024x1024.h5', 'r') as f:\n",
    "    barrel_corrector = dict()\n",
    "    for channel in channels:\n",
    "        barrel_corrector[channel] = f[channel][:]\n",
    "\n",
    "barrel_stack = np.stack(barrel_corrector.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_interact(avg, colorbar=True,\n",
    "               slider_labels=['channel'], slider_value_names=[channels])\n",
    "image_interact(normalized_avg, colorbar=True,\n",
    "               slider_labels=['channel'], slider_value_names=[channels])\n",
    "image_interact(var, colorbar=True,\n",
    "               slider_labels=['channel'], slider_value_names=[channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_avg = (avg-550) / (avg-550).mean((-1, -2), keepdims=True)\n",
    "normalized_var = var / var.mean((-1, -2), keepdims=True)\n",
    "image_interact(normalized_var / normalized_avg, colorbar=True, \n",
    "               slider_labels=['channel'], slider_value_names=[channels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old method to compute corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = []\n",
    "base_dir = '/export/home/rremme/Datasets/antibodies/covid-data-vibor/'\n",
    "for directory in tqdm(os.listdir(base_dir)):\n",
    "    if 'test_images' in directory:\n",
    "        print('skipping test images')\n",
    "        continue\n",
    "    print(directory)\n",
    "    averages.append(average_over_tiff_directory(os.path.join(base_dir, directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_averages = np.stack(filter(lambda a: a.shape[0] == 3, averages))\n",
    "blurred_averages = [blur(a, 50) for a in new_averages]\n",
    "mean_blurred_averages = np.stack(blurred_averages).mean(0)\n",
    "mean_blurred_averages -= average_bg\n",
    "mean_blurred_averages /= mean_blurred_averages.mean((-1, -2), keepdims=True)\n",
    "[image_interact(b, title=f'Channel {i}', colorbar=True, vmin=0, figsize=(5, 5)) for i, b in enumerate(mean_blurred_averages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('barrel_correction/barrel_corrector.h5', 'w') as f:\n",
    "    f.create_dataset('offset', data=average_bg, compression='gzip')\n",
    "    f.create_dataset('divisor', data=mean_blurred_averages, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Barrel correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'too few control cells': '1',\n",
    "    'too small fraction of control cells': '2',\n",
    "    'too low infected cell intensity': '3'}\n",
    "sum(list(map(len, d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all((False for _ in (1,2,3))):\n",
    "    print('Turue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barrel_correct(raw):\n",
    "    return (raw - average_bg) / mean_blurred_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_dir = '/export/home/rremme/Datasets/antibodies/covid-data-vibor/'\n",
    "files = sum([[os.path.join(tiff_dir, subdir, d) for d in os.listdir(os.path.join(tiff_dir, subdir))] for subdir in os.listdir(tiff_dir)], initial=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show 10 random corrections\n",
    "for f in np.random.choice(files, 10):\n",
    "    img = tifffile.imread(f)\n",
    "    if img.shape[0] == 4:\n",
    "        img = img[[0, 1, 2]]\n",
    "    corrected = barrel_correct(img)\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
    "    for c in range(3):\n",
    "        im = ax[0, c].imshow(img[c])\n",
    "        fig.colorbar(im, ax=ax[0, c])\n",
    "        im = ax[1, c].imshow(corrected[c])\n",
    "        fig.colorbar(im, ax=ax[1, c])\n",
    "    plt.suptitle(f'correcting {f}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
